import torch
import gradio as gr
from unsloth import FastLanguageModel
from transformers import TextStreamer
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain import HuggingFaceHub
import textwrap

device = "cuda" if torch.cuda.is_available() else "cpu"

max_seq_length = 256
dtype = None
load_in_4bit = True

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="anishkarnik/lora_model",
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
)

text_streamer = TextStreamer(tokenizer)

import gradio as gr
import re
FastLanguageModel.for_inference(model)
def classify_email(email_text):
    alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

    ### Instruction:
    {}

    ### Input:
    {}

    ### Response:
    """
    inputs = tokenizer(
        [
            alpaca_prompt.format(
                "Given an email, classify the type of email. It belongs to one of the following categories: Student inquiries / Academic collaboration inquiries / Corporate inquiries",
                email_text,
                ""
            )
        ], return_tensors="pt"
    ).to("cuda" if torch.cuda.is_available() else "cpu")

    answer = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)
    decoded_answer = tokenizer.decode(answer[0], skip_special_tokens=True)
    response_start = decoded_answer.find("### Response:")
    if response_start != -1:
        return decoded_answer[response_start + len("### Response:"):].strip()
    else:
        return "Classification error"

# Function to handle RAG response for student inquiries
def wrap_text_preserve_newlines(text, width=110):
    lines = text.split('\n')
    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]
    return '\n'.join(wrapped_lines)

loader = TextLoader('anirban.txt')  
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

embeddings = HuggingFaceEmbeddings()
db = FAISS.from_documents(docs, embeddings)

llm = HuggingFaceHub(repo_id="mistralai/Mistral-7B-v0.1", model_kwargs={"temperature": 0.1, "max_length": 200})
chain = load_qa_chain(llm, chain_type="stuff")

def rag_response(query):
    docs = db.similarity_search(query)
    response = chain.run(input_documents=docs, question=query)
    my_text= wrap_text_preserve_newlines(response)
    start_index = my_text.find("Helpful Answer:") + len("Helpful Answer: ")
    response = my_text[start_index:].strip()


    response = re.split(r'\nQuestion:', response)[0].strip()

 
    sentences = response.split('. ')
    unique_sentences = set(sentences)

    
    cleaned_response = '. '.join(unique_sentences).strip() + '.' if unique_sentences else ''

    cleaned_response = cleaned_response.replace('\n', ' ')
    return cleaned_response


def process_email(email_input):
    
    email_category = classify_email(email_input)

    
    if "student inquiries" in email_category.lower():
        response = rag_response(email_input)
        return email_category, response
    else:
        return email_category, "This is an autogenerated email. Your email has been forwarded to the HOD."

iface = gr.Interface(
    fn=process_email,
    inputs=gr.Textbox(label="Email Input"),
    outputs=[
        gr.Textbox(label="Email Type"),
        gr.Textbox(label="Response"),
    ],
    title="Email Classification and Response System",
    description="Classifies emails into three categories and provides a response for student inquiries."
)

iface.launch()
